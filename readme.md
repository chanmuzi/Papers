# Reasoning

|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.10|[Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://arxiv.org/abs/2310.18338)|Prompt Decomposition|IIT Delhi, India|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Reasoning/Small%20Language%20Models%20Fine-tuned%20to%20Coordinate%20Larger%20Language%20Models%20improve%20Complex%20Reasoning.md)|EMNLP 2023|
|2024.01|[LLMs cannot find reasoning errors, but can correct them!](https://arxiv.org/abs/2311.08516)|CoT|Google Research|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Reasoning/LLMs%20cannot%20find%20reasoning%20errors%2C%20but%20can%20correct%20them!.md)||




---
# RAG (Retrieval Augmented Generation)
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2021.10|[BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models](https://arxiv.org/abs/2104.08663)|IR Benchmark|UKP-TUDA|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/BEIR%3A%20A%20Heterogeneous%20Benchmark%20for%20Zero-shot%20Evaluation%20of%20Information%20Retrieval%20Models.md)|NeurIPS 2021|
|2023.02|[Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)|RAG, Survey|Meta|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Augmented%20Language%20Models%3A%20a%20Survey.md)||
|2023.10|[Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://arxiv.org/abs/2304.09542)|Re-Ranking|Baidu|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Is%20ChatGPT%20Good%20at%20Search%3F%20Investigating%20Large%20Language%20Models%20as%20Re-Ranking%20Agents.md)||
|2023.10|[Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://arxiv.org/abs/2303.08559)|Reranking|Nanyang Technological University|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Large%20Language%20Model%20Is%20Not%20a%20Good%20Few-shot%20Information%20Extractor%2C%20but%20a%20Good%20Reranker%20for%20Hard%20Samples!.md)||
|2023.12|[RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/abs/2312.02724)|Reranking|University of Waterloo|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/RankZephyr:%20Effective%20and%20Robust%20Zero-Shot%20Listwise%20Reranking%20is%20a%20Breeze!.md)||
|2024.01|[A Survey on Evaluation of Large Language Models](https://dl.acm.org/doi/abs/10.1145/3641289)|Evaluation, Survey|...|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/A%20Survey%20on%20Evaluation%20of%20Large%20Language%20Models.md)|ACM|
|2024.01|[The Power of Noise: Redefining Retrieval for RAG Systems](https://arxiv.org/abs/2401.14887)|Noise, RAG|...|[Makrdown](https://github.com/chanmuzi/Papers/blob/main/RAG/The%20Power%20of%20Noise%3A%20Redefining%20Retrieval%20for%20RAG%20Systems.md)|ACM|




---
# Training
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.12|[Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390)|superhuman model|OpenAI|[Blog](https://chanmuzi.tistory.com/469)||
|2024.01|[Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335)|Self-Play|...|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Self-Play%20Fine-Tuning%20Converts%20Weak%20Language%20Models%20to%20Strong%20Language%20Models.md)||
|2024.01|[Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491)|Knowledge Fusion|Tencent AI Lab|[Blog](https://chanmuzi.tistory.com/470)||
|2023.12|[Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations](https://aclanthology.org/2023.emnlp-main.753/)|Knowledge Distillation|Zoom Video Communications|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Select%2C%20Prompt%2C%20Filter%3A%20Distilling%20Large%20Language%20Models%20for%20Summarizing%20Conversations.md)|EMNLP 2023|
|2023.12|[Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/abs/2312.06585)|Self-Training, ReST|Google DeepMind|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Beyond%20Human%20Data%3A%20Scaling%20Self-Training%20for%20Problem-Solving%20with%20Language%20Models.md)||
|2024.01|[Can AI Assistants Know What They Don't Know?](https://arxiv.org/abs/2401.13275)|Hallucination|Fudan University|[Blog](https://chanmuzi.tistory.com/471)||
|2024.01|[Tuning Language Models by Proxy](https://arxiv.org/abs/2401.08565)|Proxy-tuning|Allen Institue of AI|[Blog](https://chanmuzi.tistory.com/472)||

---
# Prompting
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)|Meta-Prompting|Stanford University, OpenAI|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Prompting/Meta-Prompting%3A%20Enhancing%20Language%20Models%20with%20Task-Agnostic%20Scaffolding.md)||



---
# Compression
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024)|Model Compression|Microsoft Research|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Compression/SliceGPT%3A%20Compress%20Large%20Language%20Models%20by%20Deleting%20Rows%20and%20Columns.md)||
