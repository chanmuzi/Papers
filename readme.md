# LLM (Large Language Model)
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.02|[OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838)|Open Source, OLMo|AI2|[Makrdown](https://github.com/chanmuzi/Papers/blob/main/LLM/OLMo%3A%20Accelerating%20the%20Science%20of%20Language%20Models.md)||
|2024.01|[Orion-14B: Open-source Multilingual Large Language Models](https://arxiv.org/abs/2401.12246)|Open Source, MLLM|OrionStar Inc.|[Markdown](https://github.com/chanmuzi/Papers/blob/main/LLM/Orion-14B%3A%20Open-source%20Multilingual%20Large%20Language%20Models.md)||
|2023.04|[Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062)|Attention, RMT|Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia|[Blog](https://chanmuzi.tistory.com/283)||
|2023.03|[GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)|GPT-4|OpenAI|[Blog](https://chanmuzi.tistory.com/190)||
|2019.05|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)|BERT|Google AI Language|[Blog](https://chanmuzi.tistory.com/204)|NAACL-HLT 2019|

---


# Reasoning
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.02|[A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains](https://arxiv.org/abs/2402.00559)|Benchmark, REVEAL|Google|[Blog](https://chanmuzi.tistory.com/478)|
|2024.01|[LLMs cannot find reasoning errors, but can correct them!](https://arxiv.org/abs/2311.08516)|CoT|Google Research|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Reasoning/LLMs%20cannot%20find%20reasoning%20errors%2C%20but%20can%20correct%20them!.md)||
|2023.10|[Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning](https://arxiv.org/abs/2310.18338)|Prompt Decomposition|IIT Delhi, India|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Reasoning/Small%20Language%20Models%20Fine-tuned%20to%20Coordinate%20Larger%20Language%20Models%20improve%20Complex%20Reasoning.md)|EMNLP 2023|
|2023.04|[Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System](https://arxiv.org/abs/2304.13343)|Self-Controlled Memory (SCM)|ByteDance AI Lab|[Blog](https://chanmuzi.tistory.com/296)||
|2023.04|[PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales](https://arxiv.org/abs/2211.01562)|Rationale|University of Southern California|[Blog](https://chanmuzi.tistory.com/476)|ICLR 2023|
|2023.04|[Answering Questions by Meta-Reasoning over Multiple Chains of Thought](https://arxiv.org/abs/2304.13007)|CoT, Meta-Reasoning|AI2|[Blog](https://chanmuzi.tistory.com/285)|EMNLP 2023|


---
# RAG (Retrieval Augmented Generation)
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[A Survey on Evaluation of Large Language Models](https://dl.acm.org/doi/abs/10.1145/3641289)|Evaluation, Survey|...|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/A%20Survey%20on%20Evaluation%20of%20Large%20Language%20Models.md)|ACM|
|2024.01|[The Power of Noise: Redefining Retrieval for RAG Systems](https://arxiv.org/abs/2401.14887)|Noise, RAG|...|[Makrdown](https://github.com/chanmuzi/Papers/blob/main/RAG/The%20Power%20of%20Noise%3A%20Redefining%20Retrieval%20for%20RAG%20Systems.md)|ACM|
|2024.01|[Corrective Retrieval Augmented Generation](https://arxiv.org/abs/2401.15884)|RAG, CRAG|University of Science and Technology of Chian, <br> Google Research|[Blog](https://chanmuzi.tistory.com/480)||
|2023.12|[RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/abs/2312.02724)|Reranking|University of Waterloo|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/RankZephyr:%20Effective%20and%20Robust%20Zero-Shot%20Listwise%20Reranking%20is%20a%20Breeze!.md)||
|2023.10|[Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://arxiv.org/abs/2304.09542)|Re-Ranking|Baidu|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Is%20ChatGPT%20Good%20at%20Search%3F%20Investigating%20Large%20Language%20Models%20as%20Re-Ranking%20Agents.md)||
|2023.10|[Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!](https://arxiv.org/abs/2303.08559)|Reranking|Nanyang Technological University|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Large%20Language%20Model%20Is%20Not%20a%20Good%20Few-shot%20Information%20Extractor%2C%20but%20a%20Good%20Reranker%20for%20Hard%20Samples!.md)||
|2023.10|[Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511)|Self, RAG|University of Washington, AI2, IBM Research AI|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Self-RAG%3A%20Learning%20to%20Retrieve%2C%20Generate%2C%20and%20Critique%20through%20Self-Reflection.md)||
|2023.02|[Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)|RAG, Survey|Meta|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/Augmented%20Language%20Models%3A%20a%20Survey.md)||
|2021.10|[BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models](https://arxiv.org/abs/2104.08663)|IR Benchmark|UKP-TUDA|[Markdown](https://github.com/chanmuzi/Papers/blob/main/RAG/BEIR%3A%20A%20Heterogeneous%20Benchmark%20for%20Zero-shot%20Evaluation%20of%20Information%20Retrieval%20Models.md)|NeurIPS 2021|



---
# Training
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[Can AI Assistants Know What They Don't Know?](https://arxiv.org/abs/2401.13275)|Hallucination|Fudan University|[Blog](https://chanmuzi.tistory.com/471)||
|2024.01|[Tuning Language Models by Proxy](https://arxiv.org/abs/2401.08565)|Proxy-tuning|Allen Institue of AI|[Blog](https://chanmuzi.tistory.com/472)||
|2024.01|[Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335)|Self-Play|...|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Self-Play%20Fine-Tuning%20Converts%20Weak%20Language%20Models%20to%20Strong%20Language%20Models.md)||
|2024.01|[Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491)|Knowledge Fusion|Tencent AI Lab|[Blog](https://chanmuzi.tistory.com/470)||
|2023.12|[Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390)|superhuman model|OpenAI|[Blog](https://chanmuzi.tistory.com/469)||
|2023.12|[Select, Prompt, Filter: Distilling Large Language Models for Summarizing Conversations](https://aclanthology.org/2023.emnlp-main.753/)|Knowledge Distillation|Zoom Video Communications|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Select%2C%20Prompt%2C%20Filter%3A%20Distilling%20Large%20Language%20Models%20for%20Summarizing%20Conversations.md)|EMNLP 2023|
|2023.12|[Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models](https://arxiv.org/abs/2312.06585)|Self-Training, ReST|Google DeepMind|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Training/Beyond%20Human%20Data%3A%20Scaling%20Self-Training%20for%20Problem-Solving%20with%20Language%20Models.md)||
|2023.10|[SELF: Self-Evolution with Language Feedback](https://arxiv.org/abs/2310.00533)|SELF|The University of Hong Kong|[Blog](https://chanmuzi.tistory.com/481)||
|2023.03|[oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes](https://arxiv.org/abs/2303.17612)|RoBERTa, Knowledge Distillation|Department of Computer Science, the University of Illinois Urbana-Champaign|[Blog](https://chanmuzi.tistory.com/284)||


---
# Prompting
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding](https://arxiv.org/abs/2401.12954)|Meta-Prompting|Stanford University, OpenAI|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Prompting/Meta-Prompting%3A%20Enhancing%20Language%20Models%20with%20Task-Agnostic%20Scaffolding.md)||


---
# Compression
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.01|[SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024)|Model Compression|Microsoft Research|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Compression/SliceGPT%3A%20Compress%20Large%20Language%20Models%20by%20Deleting%20Rows%20and%20Columns.md)||

---
# Tool, Agent
|Date|Title(arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.04|[Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models](https://arxiv.org/abs/2304.09842)|Tools, Chameleon|University of California, Los Angeles, Microsoft Research|[Blog](https://chanmuzi.tistory.com/301)|NeurIPS 2023|


---
# Evaluation / Benchmark / Dataset
|Date|Title (arxiv)|Keyword|Affiliation|Note|Conference|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.02|[LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)|NLG, Evaluation|Peking University|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Evaluation/LLM-based%20NLG%20Evaluation%3A%20Current%20Status%20and%20Challenges.md)||
|2024.02|[Can Large Language Models Understand Context?](https://arxiv.org/abs/2402.00858)|Benchmark, In-Context Learning|Apple|[Markdown](https://github.com/chanmuzi/Papers/blob/main/LLM/Can%20Large%20Language%20Models%20Understand%20Context%3F.md)|Findings of EACL 2024|
|2024.01|[Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research](https://arxiv.org/abs/2402.00159)|Dolma, Dataset, Pretraining|AI2|[Markdown](https://github.com/chanmuzi/Papers/blob/main/Evaluation%2C%20Benchmark%2C%20Dataset/Dolma%3A%20an%20Open%20Corpus%20of%20Three%20Trillion%20Tokens%20for%20Language%20Model%20Pretraining%20Research.md)||
|2023.05|[Do Large Language Models Know What They Don't Know?](https://arxiv.org/abs/2305.18153)|Self-Knowledge, SelfAware (Dataset)|School of Computer Science, Fudan University|[Markdown](https://github.com/chanmuzi/Papers/tree/main/Evaluation,%20Benchmark,%20Dataset)||
|2023.04|[Evaluating Verifiability in Generative Search Engines](https://arxiv.org/abs/2304.09848)|Generative Search Engine|Stanford University|[Blog](https://chanmuzi.tistory.com/302)||
|2016.10|[SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250)|SQUAD, QA, Benchmark|Stanford University|[Blog](https://chanmuzi.tistory.com/171)||
